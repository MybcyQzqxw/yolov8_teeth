#!/usr/bin/env python3
"""
æµ‹è¯•æ¯ç±»åˆ«è¯„ä¼°åŠŸèƒ½çš„ä¿®å¤
"""
import os
import sys
sys.path.append('.')
from utils.yolov8.per_class_evaluator import evaluate_and_visualize_per_class

def test_per_class_evaluation():
    # ä½¿ç”¨å·²æœ‰çš„è®­ç»ƒç»“æœè¿›è¡Œæµ‹è¯•
    model_path = 'outputs/yolov8/train_yolov8m_1ep_2025_08_13_13_49_34/weights/weights/best.pt'
    data_path = 'preprocessed_datasets/yolov8/data.yaml'
    class_names = ['Caries', 'Cavity', 'Crack', 'Tooth']
    output_dir = 'outputs/yolov8/train_yolov8m_1ep_2025_08_13_13_49_34/logs'

    print('ğŸ§ª æµ‹è¯•æ¯ç±»åˆ«è¯„ä¼°åŠŸèƒ½...')
    print(f'ğŸ“¦ æ¨¡å‹è·¯å¾„: {model_path}')
    print(f'ğŸ“Š æ•°æ®è·¯å¾„: {data_path}')
    print(f'ğŸ“ è¾“å‡ºç›®å½•: {output_dir}')

    if os.path.exists(model_path):
        print('âœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼Œå¼€å§‹è¯„ä¼°...')
        try:
            metrics = evaluate_and_visualize_per_class(model_path, data_path, class_names, output_dir)
            if metrics:
                print('âœ… æ¯ç±»åˆ«è¯„ä¼°æˆåŠŸå®Œæˆ!')
                for class_name, class_metrics in metrics.items():
                    f1_score = class_metrics.get('f1_score', 0)
                    print(f'   {class_name}: F1={f1_score:.3f}')
                return True
            else:
                print('âš ï¸ è¯„ä¼°è¿”å›ç©ºç»“æœ')
                return False
        except Exception as e:
            print(f'âŒ è¯„ä¼°å¤±è´¥: {e}')
            return False
    else:
        print('âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨')
        return False

if __name__ == '__main__':
    success = test_per_class_evaluation()
    exit(0 if success else 1)
