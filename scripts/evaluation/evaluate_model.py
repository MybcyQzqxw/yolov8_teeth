#!/usr/bin/env python3
"""
YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒå¢å¼ºæŒ‡æ ‡åˆ†æï¼ŒåŒ…æ‹¬F1-Scoreã€IoUã€æ··æ·†çŸ©é˜µã€æ¯ç±»åˆ«mAPç­‰
"""

import argparse
import os
import sys
import yaml

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from utils.yolov8.metrics import (
    plot_enhanced_metrics, 
    generate_metrics_report, 
    enhanced_metrics_analysis
)
from utils.yolov8.per_class_evaluator import evaluate_and_visualize_per_class


def main():
    parser = argparse.ArgumentParser(
        description="YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹å¢å¼ºè¯„ä¼°è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨è®­ç»ƒç»“æœè¿›è¡Œè¯„ä¼°
  python scripts/evaluation/evaluate_model.py --results results.csv --output ./evaluation_output
  
  # ä½¿ç”¨æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œå®Œæ•´è¯„ä¼°
  python scripts/evaluation/evaluate_model.py --model best.pt --data data.yaml --output ./evaluation_output
  
  # ä»…åˆ†æå·²æœ‰çš„è®­ç»ƒç»“æœ
  python scripts/evaluation/evaluate_model.py --results outputs/train_xxx/weights/results.csv
        """)
    
    # è¾“å…¥å‚æ•°
    parser.add_argument('--results', '-r', type=str, 
                        help="è®­ç»ƒç»“æœCSVæ–‡ä»¶è·¯å¾„ (results.csv)")
    parser.add_argument('--model', '-m', type=str, 
                        help="è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.ptæ–‡ä»¶)")
    parser.add_argument('--data', '-d', type=str, 
                        help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„ (data.yaml)")
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--output', '-o', type=str, default="./evaluation_output",
                        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./evaluation_output)")
    parser.add_argument('--split', type=str, default="val", choices=['train', 'val', 'test'],
                        help="è¯„ä¼°æ•°æ®é›†åˆ†å‰² (é»˜è®¤: val)")
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument('--skip-per-class', action='store_true',
                        help="è·³è¿‡æ¯ç±»åˆ«è¯¦ç»†è¯„ä¼° (éœ€è¦æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶)")
    parser.add_argument('--classes', type=str, nargs='+', 
                        default=['Caries', 'Cavity', 'Crack', 'Tooth'],
                        help="ç±»åˆ«åç§°åˆ—è¡¨ (é»˜è®¤: Caries Cavity Crack Tooth)")
    
    args = parser.parse_args()
    
    # å‚æ•°éªŒè¯
    if not args.results and not (args.model and args.data):
        print("âŒ é”™è¯¯: å¿…é¡»æä¾›ä»¥ä¸‹å‚æ•°ä¹‹ä¸€:")
        print("   1. --results: è®­ç»ƒç»“æœCSVæ–‡ä»¶")
        print("   2. --model å’Œ --data: æ¨¡å‹æ–‡ä»¶å’Œæ•°æ®é…ç½®æ–‡ä»¶")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)
    print(f"ğŸ“ è¯„ä¼°ç»“æœå°†ä¿å­˜è‡³: {args.output}")
    
    # å¤„ç†ç±»åˆ«åç§°
    class_names = args.classes
    
    # å¦‚æœæä¾›äº†æ•°æ®æ–‡ä»¶ï¼Œå°è¯•ä»ä¸­è¯»å–ç±»åˆ«åç§°
    if args.data and os.path.exists(args.data):
        try:
            with open(args.data, 'r', encoding='utf-8') as f:
                data_config = yaml.safe_load(f)
                if 'names' in data_config:
                    class_names = data_config['names']
                    print(f"ğŸ“‹ ä»æ•°æ®æ–‡ä»¶è¯»å–ç±»åˆ«: {class_names}")
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ•°æ®æ–‡ä»¶ç±»åˆ«å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«: {e}")
    
    print(f"ğŸ·ï¸ è¯„ä¼°ç±»åˆ«: {class_names}")
    
    # 1. å¦‚æœæä¾›äº†results.csvï¼Œè¿›è¡ŒåŸºäºè®­ç»ƒç»“æœçš„è¯„ä¼°
    if args.results:
        if not os.path.exists(args.results):
            print(f"âŒ æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶: {args.results}")
            return
        
        print("ğŸ“Š å¼€å§‹åŸºäºè®­ç»ƒç»“æœçš„å¢å¼ºæŒ‡æ ‡åˆ†æ...")
        
        # ç”Ÿæˆå¢å¼ºæŒ‡æ ‡å›¾è¡¨
        enhanced_plot_path = os.path.join(args.output, "enhanced_metrics_analysis.png")
        metrics = plot_enhanced_metrics(args.results, enhanced_plot_path, class_names)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = os.path.join(args.output, "metrics_report.md")
        generate_metrics_report(args.results, class_names, report_path)
        
        print(f"âœ… åŸºäºè®­ç»ƒç»“æœçš„è¯„ä¼°å®Œæˆ:")
        print(f"   - å¢å¼ºæŒ‡æ ‡å›¾è¡¨: {enhanced_plot_path}")
        print(f"   - è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡æ‘˜è¦
        if metrics:
            print(f"ğŸ¯ å…³é”®æŒ‡æ ‡æ‘˜è¦:")
            print(f"   - F1-Score: {metrics.get('f1_score', 0):.3f}")
            print(f"   - Precision: {metrics.get('precision', 0):.3f}")
            print(f"   - Recall: {metrics.get('recall', 0):.3f}")
            print(f"   - mAP@0.5: {metrics.get('map50', 0):.3f}")
            print(f"   - IoUè´¨é‡: {metrics.get('avg_iou_at_0.5', 0):.3f}")
    
    # 2. å¦‚æœæä¾›äº†æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶ï¼Œè¿›è¡Œå®Œæ•´çš„æ¯ç±»åˆ«è¯„ä¼°
    if args.model and args.data and not args.skip_per_class:
        if not os.path.exists(args.model):
            print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {args.model}")
            return
        if not os.path.exists(args.data):
            print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {args.data}")
            return
        
        print(f"ğŸ” å¼€å§‹æ¯ç±»åˆ«è¯¦ç»†è¯„ä¼° ({args.split}æ•°æ®é›†)...")
        
        # è¿è¡Œæ¯ç±»åˆ«è¯„ä¼°
        per_class_metrics = evaluate_and_visualize_per_class(
            args.model, args.data, class_names, args.output
        )
        
        if per_class_metrics:
            print("âœ… æ¯ç±»åˆ«è¯¦ç»†è¯„ä¼°å®Œæˆ:")
            print(f"   - æ¯ç±»åˆ«å›¾è¡¨: {os.path.join(args.output, 'per_class_metrics.png')}")
            print(f"   - æ¯ç±»åˆ«æŠ¥å‘Š: {os.path.join(args.output, 'per_class_report.md')}")
            
            # æ˜¾ç¤ºæ¯ç±»åˆ«F1-Scoreæ‘˜è¦
            print("ğŸ† æ¯ç±»åˆ«F1-Score:")
            for class_name, metrics in per_class_metrics.items():
                print(f"   - {class_name}: {metrics.get('f1_score', 0):.3f}")
    
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {args.output}")


if __name__ == '__main__':
    main()
