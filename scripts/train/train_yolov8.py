import argparse
import os
import sys
import torch
from ultralytics import YOLO, settings

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from utils.yolov8.visualization import plot_loss_curve
from utils.yolov8.file_utils import create_output_dirs, validate_files, ensure_model_extension
from utils.yolov8.metrics import plot_enhanced_metrics, generate_metrics_report
from utils.yolov8.per_class_evaluator import evaluate_and_visualize_per_class

# é…ç½®ultralyticså°†æ¨¡å‹ä¸‹è½½åˆ°modelsæ–‡ä»¶å¤¹ï¼Œæ•°æ®é›†ä½¿ç”¨å½“å‰ç›®å½•
settings.update({
    'weights_dir': 'models',
    'datasets_dir': 'dataset',
    'runs_dir': 'outputs/dentalai'  # è®¾ç½®è¿è¡Œè¾“å‡ºç›®å½•
})

def detect_device_with_user_prompt():
    """
    æ™ºèƒ½è®¾å¤‡æ£€æµ‹å‡½æ•°ï¼Œè‡ªåŠ¨æ£€æµ‹GPUå¯ç”¨æ€§å¹¶ç»™å‡ºç”¨æˆ·å‹å¥½çš„æç¤º
    """
    print("ğŸ” æ­£åœ¨æ£€æµ‹å¯ç”¨çš„è®­ç»ƒè®¾å¤‡...")
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
        
        print(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªå¯ç”¨ GPU")
        print(f"ğŸ¯ ä½¿ç”¨ GPU è®­ç»ƒ: {gpu_name}")
        print(f"ğŸ’¾ GPU æ˜¾å­˜: {gpu_memory}GB")
        return "0"
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPUï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒ")
        print("ğŸ’¡ æç¤º: CPU è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®:")
        print("   1. å®‰è£…æ”¯æŒ CUDA çš„ PyTorch:")
        print("      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        print("   2. ç¡®ä¿ NVIDIA æ˜¾å¡é©±åŠ¨å·²æ­£ç¡®å®‰è£…")
        print("   3. æ£€æŸ¥ CUDA ç‰ˆæœ¬å…¼å®¹æ€§")
        print("   4. å‚è€ƒå®˜æ–¹æ–‡æ¡£: https://pytorch.org/get-started/locally/")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­ CPU è®­ç»ƒ
        while True:
            try:
                user_input = input("ğŸ¤” æ˜¯å¦ç»§ç»­ä½¿ç”¨ CPU è®­ç»ƒ? (y/n): ").lower().strip()
                if user_input in ['y', 'yes', 'æ˜¯', '']:
                    print("ğŸ“ ç»§ç»­ä½¿ç”¨ CPU è®­ç»ƒ...")
                    return "cpu"
                elif user_input in ['n', 'no', 'å¦']:
                    print("ğŸšª å·²å–æ¶ˆè®­ç»ƒï¼Œè¯·é…ç½® GPU ç¯å¢ƒåé‡è¯•")
                    return None
                else:
                    print("â“ è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")
            except (KeyboardInterrupt, EOFError):
                print("\nğŸšª è®­ç»ƒå·²å–æ¶ˆ")
                return None

def main():
    parser = argparse.ArgumentParser(
        description="YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python train.py                                      # é›¶é…ç½®è®­ç»ƒ(æ¨èæ–°æ‰‹)
  python train.py -e 100                               # ä»…æŒ‡å®šè®­ç»ƒè½®æ•°
  python train.py -m yolov8n -e 50                     # å°æ¨¡å‹å¿«é€Ÿè®­ç»ƒ
  python train.py -m yolov8s -e 100 -b 32              # ä¸­ç­‰è§„æ¨¡è®­ç»ƒ
  python train.py -m yolov8x -b -1 --device 0          # å¤§æ¨¡å‹è‡ªåŠ¨æ‰¹é‡å¤§å°
        """)
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--model', '-m', type=str, default="yolov8m",
                        help="æ¨¡å‹ç±»å‹: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x (é»˜è®¤: yolov8m)")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', '-e', type=int, default=30,
                        help="è®­ç»ƒè½®æ•° (é»˜è®¤: 30)")
    parser.add_argument('--batch', '-b', type=int, default=16,
                        help="æ‰¹é‡å¤§å°: æ­£æ•´æ•°æˆ–-1(è‡ªåŠ¨æ‰¹é‡å¤§å°) (é»˜è®¤: 16)")
    parser.add_argument('--imgsz', type=int, default=640,
                        help="è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤: 640)")
    parser.add_argument('--device', type=str, default="auto",
                        help="è®­ç»ƒè®¾å¤‡: auto, cpu, 0, 1, 2, 3... (é»˜è®¤: auto)")
    
    # æ•°æ®å’Œè¾“å‡º
    parser.add_argument('--data_dir', '-d', type=str, default="./preprocessed_datasets/dentalai",
                        help="è®­ç»ƒæ•°æ®æ–‡ä»¶å¤¹ï¼ŒåŒ…å« data.yaml (é»˜è®¤: ./preprocessed_datasets/dentalai)")
    parser.add_argument('--output_dir', '-o', type=str, default="./outputs/dentalai",
                        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./outputs/dentalai)")
    
    # è®­ç»ƒé€‰é¡¹
    parser.add_argument('--patience', type=int, default=30,
                        help="æ—©åœè€å¿ƒå€¼ï¼Œå¤šå°‘è½®æ— æ”¹å–„ååœæ­¢ (é»˜è®¤: 30)")
    parser.add_argument('--save_period', type=int, default=10,
                        help="ä¿å­˜æ£€æŸ¥ç‚¹çš„é—´éš”è½®æ•° (é»˜è®¤: 10)")
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--nolog', action='store_true',
                        help="ç¦ç”¨æ—¥å¿—è¾“å‡ºå’Œå¯è§†åŒ–å›¾è¡¨")
    parser.add_argument('--verbose', action='store_true',
                        help="æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒä¿¡æ¯")
    
    args = parser.parse_args()

    # å¤„ç†æ¨¡å‹æ–‡ä»¶å
    model_file = ensure_model_extension(args.model)
    
    # éªŒè¯æ‰¹é‡å¤§å°
    if args.batch <= 0 and args.batch != -1:
        raise ValueError("æ‰¹é‡å¤§å°å¿…é¡»ä¸ºæ­£æ•´æ•°æˆ–-1(è‡ªåŠ¨)")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
    data_yaml = os.path.join(args.data_dir, "data.yaml")
    validate_files(model_file, data_yaml)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    base_dir, weights_dir, logs_dir = create_output_dirs(
        args.model, args.epochs, args.output_dir, enable_logs=not args.nolog
    )

    # æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œç”¨æˆ·æç¤º
    if args.device == "auto":
        device = detect_device_with_user_prompt()
        if device is None:  # ç”¨æˆ·é€‰æ‹©å–æ¶ˆè®­ç»ƒ
            return
    else:
        device = args.device
        print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹")
    print(f"   ğŸ“¦ æ¨¡å‹: {model_file}")
    print(f"   ğŸ“Š è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"   ğŸ“ æ‰¹é‡å¤§å°: {args.batch}")
    print(f"   ğŸ–¼ï¸  å›¾åƒå°ºå¯¸: {args.imgsz}")
    print(f"   ğŸ’» è®­ç»ƒè®¾å¤‡: {device}")
    print(f"   ğŸ“ æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"   ğŸ’¾ è¾“å‡ºç›®å½•: {base_dir}")
    print(f"   ğŸ“ˆ æ—¥å¿—è®°å½•: {'å…³é—­' if args.nolog else 'å¼€å¯'}")

    # å¼€å§‹è®­ç»ƒ
    try:
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿æ¨¡å‹ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®
        os.environ['YOLO_CONFIG_DIR'] = os.path.join(os.getcwd(), 'models')
        
        model = YOLO(model_file)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ!")
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        result = model.train(
            data=data_yaml,
            epochs=args.epochs,
            batch=args.batch,
            imgsz=args.imgsz,
            device=device,  # ä½¿ç”¨æ™ºèƒ½æ£€æµ‹çš„è®¾å¤‡
            project=base_dir,
            name="weights",
            exist_ok=True,
            patience=args.patience,
            save_period=args.save_period,
            verbose=args.verbose,
            amp=False  # ç¦ç”¨AMPä»¥é¿å…è‡ªåŠ¨ä¸‹è½½yolo11n.pt
        )
        
    except ConnectionError as e:
        print(f"âŒ ç½‘ç»œè¿æ¥é”™è¯¯: {e}")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°é¡¹ç›®ç›®å½•:")
        print(f"      https://github.com/ultralytics/assets/releases/download/v8.2.0/{model_file}")
        print("   3. æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹: python train.py -m yolov8n")
        return
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return

    # é»˜è®¤ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨ï¼ˆé™¤éæ˜¾å¼æŒ‡å®š --nologï¼‰
    if not args.nolog:
        # YOLOv8å¯èƒ½åœ¨åµŒå¥—çš„weightsç›®å½•ä¸­ä¿å­˜results.csv
        results_csv = os.path.join(base_dir, "weights", "results.csv")
        alt_results_csv = os.path.join(base_dir, "weights", "weights", "results.csv")
        
        # æ£€æŸ¥results.csvæ–‡ä»¶ä½ç½®
        if os.path.exists(results_csv):
            csv_path_to_use = results_csv
        elif os.path.exists(alt_results_csv):
            csv_path_to_use = alt_results_csv
        else:
            csv_path_to_use = None
            
        if csv_path_to_use:
            # è¯»å–ç±»åˆ«åç§°
            import yaml
            try:
                with open(data_yaml, 'r', encoding='utf-8') as f:
                    data_config = yaml.safe_load(f)
                    class_names = data_config.get('names', ['Unknown'])
            except:
                class_names = ['Caries', 'Cavity', 'Crack', 'Tooth']  # é»˜è®¤ç±»åˆ«
            
            print("ğŸ“Š å¼€å§‹ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š...")
            
            # 1. ç”Ÿæˆä¼ ç»Ÿçš„è®­ç»ƒåˆ†æå›¾è¡¨
            traditional_plot_path = os.path.join(logs_dir, "training_analysis.png")
            plot_loss_curve(csv_path_to_use, traditional_plot_path)
            
            # 2. ç”Ÿæˆå¢å¼ºçš„æŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨
            enhanced_plot_path = os.path.join(logs_dir, "enhanced_metrics_analysis.png")
            metrics = plot_enhanced_metrics(csv_path_to_use, enhanced_plot_path, class_names)
            
            # 3. ç”Ÿæˆè¯¦ç»†çš„æŒ‡æ ‡æŠ¥å‘Š
            report_path = os.path.join(logs_dir, "metrics_report.md")
            generate_metrics_report(csv_path_to_use, class_names, report_path)
            
            # 4. è¿›è¡Œæ¯ç±»åˆ«è¯¦ç»†è¯„ä¼°å¹¶ä¿å­˜åˆ°CSV
            # YOLOv8 åˆ›å»ºåµŒå¥—çš„weightsç›®å½•ç»“æ„: project/name/weights/best.pt
            best_model_path = os.path.join(base_dir, "weights", "weights", "best.pt")
            # å¤‡ç”¨è·¯å¾„ï¼Œä»¥é˜²ç»“æ„ä¸åŒ
            alt_best_model_path = os.path.join(base_dir, "weights", "best.pt")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(best_model_path):
                model_path_to_use = best_model_path
                print(f"ğŸ” å¼€å§‹æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡è¯„ä¼°... (ä½¿ç”¨: {model_path_to_use})")
            elif os.path.exists(alt_best_model_path):
                model_path_to_use = alt_best_model_path
                print(f"ğŸ” å¼€å§‹æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡è¯„ä¼°... (ä½¿ç”¨: {model_path_to_use})")
            else:
                model_path_to_use = None
                print("âš ï¸ æœªæ‰¾åˆ°best.ptæ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡æ¯ç±»åˆ«è¯„ä¼°")
                print(f"   ğŸ” æŸ¥æ‰¾è·¯å¾„1: {best_model_path}")
                print(f"   ğŸ” æŸ¥æ‰¾è·¯å¾„2: {alt_best_model_path}")
            
            per_class_metrics = None
            if model_path_to_use:
                per_class_metrics = evaluate_and_visualize_per_class(
                    model_path_to_use, data_yaml, class_names, logs_dir
                )
                
                # 5. ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æ•°æ®CSVæ–‡ä»¶
                if per_class_metrics:
                    evaluation_csv_path = os.path.join(logs_dir, "complete_evaluation_metrics.csv")
                    _save_complete_evaluation_csv(metrics, per_class_metrics, class_names, evaluation_csv_path)
                    print(f"ğŸ“‹ å®Œæ•´è¯„ä¼°æ•°æ®å·²ä¿å­˜è‡³: {evaluation_csv_path}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°best.ptæ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡æ¯ç±»åˆ«è¯„ä¼°")
            
            print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å’Œå®Œæ•´è¯„ä¼°ç»“æœä¿å­˜è‡³: {base_dir}")
            print(f"ğŸ“Š è¯„ä¼°ç»“æœæ–‡ä»¶:")
            print(f"   ğŸ“ˆ è®­ç»ƒæ›²çº¿: {traditional_plot_path}")
            print(f"   ğŸ“Š å¢å¼ºåˆ†æ: {enhanced_plot_path}")
            print(f"   ğŸ“‹ æ•´ä½“æŠ¥å‘Š: {report_path}")
            
            if per_class_metrics:
                print(f"   ğŸ·ï¸  æ¯ç±»åˆ«å›¾è¡¨: {os.path.join(logs_dir, 'per_class_metrics.png')}")
                print(f"   ğŸ“Š æ¯ç±»åˆ«æŠ¥å‘Š: {os.path.join(logs_dir, 'per_class_report.md')}")
                print(f"   ğŸ“‹ å®Œæ•´è¯„ä¼°CSV: {os.path.join(logs_dir, 'complete_evaluation_metrics.csv')}")
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡æ‘˜è¦
            if metrics:
                print(f"ğŸ¯ å…³é”®æŒ‡æ ‡æ‘˜è¦:")
                print(f"   - F1-Score: {metrics.get('f1_score', 0):.3f}")
                print(f"   - Precision: {metrics.get('precision', 0):.3f}")
                print(f"   - Recall: {metrics.get('recall', 0):.3f}")
                print(f"   - mAP@0.5: {metrics.get('map50', 0):.3f}")
                print(f"   - IoUè´¨é‡: {metrics.get('avg_iou_at_0.5', 0):.3f}")
                
            # æ˜¾ç¤ºæ¯ç±»åˆ«F1-Scoreæ‘˜è¦
            if per_class_metrics:
                print(f"ğŸ† æ¯ç±»åˆ«F1-Score:")
                for class_name, class_metrics in per_class_metrics.items():
                    print(f"   - {class_name}: {class_metrics.get('f1_score', 0):.3f}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° results.csvï¼Œæ— æ³•ç”Ÿæˆè®­ç»ƒåˆ†æå›¾è¡¨")
            print(f"   ğŸ” æŸ¥æ‰¾è·¯å¾„1: {results_csv}")
            print(f"   ğŸ” æŸ¥æ‰¾è·¯å¾„2: {alt_results_csv}")
    else:
        print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜è‡³: {base_dir}")


def _save_complete_evaluation_csv(overall_metrics, per_class_metrics, class_names, save_path):
    """
    ä¿å­˜å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡åˆ°CSVæ–‡ä»¶
    
    Args:
        overall_metrics (dict): æ•´ä½“æŒ‡æ ‡
        per_class_metrics (dict): æ¯ç±»åˆ«æŒ‡æ ‡
        class_names (list): ç±»åˆ«åç§°
        save_path (str): ä¿å­˜è·¯å¾„
    """
    try:
        import pandas as pd
        
        # å‡†å¤‡æ•°æ®
        data = []
        
        # æ·»åŠ æ•´ä½“æŒ‡æ ‡è¡Œ
        overall_row = {
            'Type': 'Overall',
            'Class': 'All',
            'Precision': overall_metrics.get('precision', 0),
            'Recall': overall_metrics.get('recall', 0),
            'F1_Score': overall_metrics.get('f1_score', 0),
            'mAP_50': overall_metrics.get('map50', 0),
            'mAP_50_95': overall_metrics.get('map50_95', 0),
            'IoU_Quality_50': overall_metrics.get('avg_iou_at_0.5', 0),
            'IoU_Quality_50_95': overall_metrics.get('avg_iou_0.5_to_0.95', 0),
            'Epoch': int(overall_metrics.get('epoch', 0))
        }
        data.append(overall_row)
        
        # æ·»åŠ æ¯ç±»åˆ«æŒ‡æ ‡è¡Œ
        if per_class_metrics:
            for class_name, class_metrics in per_class_metrics.items():
                class_row = {
                    'Type': 'Per_Class',
                    'Class': class_name,
                    'Precision': class_metrics.get('precision', 0),
                    'Recall': class_metrics.get('recall', 0),
                    'F1_Score': class_metrics.get('f1_score', 0),
                    'mAP_50': class_metrics.get('ap50', 0),
                    'mAP_50_95': class_metrics.get('ap50_95', 0),
                    'IoU_Quality_50': class_metrics.get('ap50', 0),  # AP50ä½œä¸ºIoU@0.5è´¨é‡æŒ‡æ ‡
                    'IoU_Quality_50_95': class_metrics.get('ap50_95', 0),  # AP50-95ä½œä¸ºç»¼åˆIoUè´¨é‡
                    'Epoch': int(overall_metrics.get('epoch', 0))
                }
                data.append(class_row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(data)
        df.to_csv(save_path, index=False, float_format='%.4f')
        
        print(f"[âœ“] å®Œæ•´è¯„ä¼°æŒ‡æ ‡CSVå·²ä¿å­˜: {save_path}")
        
    except Exception as e:
        print(f"[!] ä¿å­˜å®Œæ•´è¯„ä¼°CSVå¤±è´¥: {e}")

if __name__ == '__main__':
    main()
